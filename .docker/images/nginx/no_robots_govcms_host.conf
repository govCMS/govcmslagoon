# Return with a `Disallow: /` for non-production URLs

if ($uri = "/robots.txt"){
  set $robots_disallow_test "isRobotstxtURI";
}
# matches *.govcms.gov.au (also multiple subdomains) except www.govcms.gov.au
if ($host ~* ^(?!www\.govcms\.gov\.au$)(?:[\w\-]+\.)+govcms\.gov\.au$){
  set $robots_disallow_test "$robots_disallow_test+isGovcmsHost";
}

if ($robots_disallow_test = "isRobotstxtURI+isGovcmsHost"){
  return 200 'User-agent: *\nDisallow: /';
}
